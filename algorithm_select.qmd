---
title: "COSC 5557: Practical Machine Learning"
subtitle: "Algorithm Selection"
author: "Abiodun Awosola"
date: "`r Sys.Date()`"
format: pdf
editor: visual
---

\textcolor{blue}{Introduction:}

The aim of this study is to determine the most suitable machine learning algorithm for predicting wine quality based on a dataset of wine attributes. We will employ a variety of machine learning algorithms commonly used for regression tasks, including logistic regression, support vector machines (SVM), decision trees, random forests, and Naïve Bayes classifier. By comparing the performance of these algorithms on the Wine Quality dataset, we aim to identify the algorithm(s) that produce the most accurate predictions.

\textcolor{blue}{Experimental Setup:}

For this experiment, we will utilize Python and R programming languages along with several libraries such as pandas, scikit-learn, and matplotlib for Python; and tidyverse, knitr, DataExplore for R. These libraries provide functions for data manipulation, machine learning algorithms, and visualization.

We will begin by loading the White Wine Quality dataset into pandas `DataFrame` and R `data.frame`.

\textcolor{blue}{Dataset Description:}

The White Wine Quality dataset contains physicochemical properties of white variants of the Portuguese "Vinho Verde" wine. Each observation represents a wine sample, and the features include various chemical properties such as acidity, pH, alcohol content, etc. The target variable is the quality rating of the white wine, which is a score between 0 and 10.

The dataset consists of approximately 4900 observations. There are no missing values in the dataset.

```{python}
#| echo: false

#Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
 
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
```

```{python}
#| echo: false
#| include: false

#Read the dataset
data=pd.read_csv(r'C:\Users\Laptop\OneDrive\Desktop\winequality-white.csv', sep=r';')

#data.head

# Check the column names
print(data.columns)
```

```{python}
#| echo: false
#| include: false
 
#Display the first five rows
print(data.head())


data.groupby('quality').count()
```

```{r, data, message=F, warning=F, echo=FALSE}
#Imports Data

wine_data <- read.table("winequality-white.csv", sep = ";",
                        check.names = TRUE, header=T)

```

```{r, message=F, warning=F, echo=F, fig.height=12, fig.width=16}
library(tidyverse)
library(knitr)


library(xtable)
#Rotates the table
wine_data1 <- xtable(t(wine_data))

#First 10 rows of data
kable(wine_data1[,1:8],
      caption = "First 8 Rows of White Wine Data")



wine_data_summary <- xtable(t(summary(wine_data)))


kable(wine_data_summary,
      caption = "Summary of White Wine Data")

# Details the data attributes
 result <- skimr::skim(wine_data)
 
 result[,c(1:3)]
```

\textcolor{blue}{Density Plots}

The density plots show white wine variables distribution.

```{r, message=F, warning=F, echo=F, fig.height=12, fig.width=16}
 DataExplorer::plot_density(wine_data,
  title = "White Wine Variables Density Plot",
  theme_config = theme(plot.title = element_text(color = "blue"))) # density plot
```

\textcolor{blue}{QQ Plots}

QQ plots show tendency of variables to be normally distributed. Most points of a variable need to be on the 45-degree straight line for the variable to be normally distributed.

```{r, message=F, warning=F, echo=F, fig.height=12, fig.width=16}
 DataExplorer::plot_qq(wine_data,
  title = "White Wine Variables QQ Plot", ncol = 4L,
  theme_config = theme(plot.title = element_text(color = "blue"))) # qq plot
```

\textcolor{blue}{Correlation Plot}

The correlation plot shows the linear relation between the variables.

```{r, message=F, warning=F, echo=F, fig.height=12, fig.width=16}
 
   
 DataExplorer::plot_correlation(wine_data,
  title = "White Wine Variables Correlation Analysis",
  theme_config = theme(plot.title = element_text(color = "blue"))) # correlation analysis
 
```

\textcolor{blue}{Boxplots}

The boxplots show white wine variables distribution by wine `quality`.

```{r, message=F, warning=F, echo=F, fig.height=12, fig.width=16}

 DataExplorer::plot_boxplot(wine_data, by = "quality", ncol = 2L,
  title = "White Wine Variables Boxplots By Wine Qualty",
  theme_config = theme(plot.title = element_text(color = "blue")))
```

The dots after the whiskers, (the horizontal end lines) represent outliers.

\textcolor{blue}{Checking for null values}

```{python}
#| echo: false

print(data.isnull().sum())
```

\textcolor{blue}{Data types in Python.}

```{python}
#| echo: false

data.dtypes

```

\textcolor{blue}{Training and Test Datasets}

Next, we will split the dataset into training and testing sets using an 80-20 split, respectively. Each machine learning algorithm is trained on the training set and the performance is evaluated on the test dataset.

```{python}
#| echo: false

# Transform data

def label_encoder(y):
    le = LabelEncoder()
    data[y] = le.fit_transform(data[y])
 
label_list = ["fixed acidity","volatile acidity","citric acid","residual sugar","chlorides","free sulfur dioxide","total sulfur dioxide","density","pH","sulphates","alcohol"]
 
for l in label_list:
    label_encoder(l)
 
#Display transformed data
data.head()


# Split the data into training and testing sets:

#Divide the dataset into independent and dependent variables
X = data.drop(["quality"],axis=1)
y = data['quality']
# Assuming y is your target variable

#Split the data into training and testing set
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,
                                               random_state=42, shuffle=True) 
```

```{python}
#| echo: false
y_train = y_train.values.reshape(-1,1)
y_test = y_test.values.reshape(-1,1)
 
print("X_train shape:",X_train.shape)
print("X_test shape:",X_test.shape)
print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

```

\textcolor{blue}{Data Preprocessing}

Here is effort to preprocess the data by standardizing the features to have zero mean and unit variance. This preprocessing step is essential for algorithms sensitive to feature scaling, such as SVM.

```{python}
#| echo: false

#Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

```

We will build all six classification models and compare their accuracy scores. The machine learning algorithms considered for this experiment are as follows:

1.  Logistic Regression
2.  KNN Classifier
3.  Support Vector Machine, (SVR)
4.  Decision Tree Regression
5.  Random Forest Regression
6.  Naïve Bayes Classifier

For each algorithm, we will use default `hyperparameters` provided by `scikit-learn`. However, if necessary, we may perform `hyperparameter` tuning using techniques such as grid search or random search to further optimize the model performance.

\textcolor{blue}{Cross–validation}

The purpose of cross–validation is to test the ability of a machine learning model to predict new data. It is also used to flag problems like overfitting or selection bias and gives insights on how the model will generalize to an independent dataset.

To test the ability of a machine learning model to predict new data, we will employ k-fold `cross-validation` with k=5. This technique partitions the dataset into k equal-sized folds, where each fold serves as the testing set once while the remaining k-1 folds are used for training.

```{python}
#| echo: false
#To store results of models, we create two dictionaries
result_dict_train = {}
result_dict_test = {}
```

\textcolor{blue}{1.  Logistic Regression}

Logistic regression is a supervised learning algorithm used for binary classification tasks. It models the probability of the input belonging to a particular class using the logistic function, which maps input features to probabilities between 0 and 1. It assumes a linear relationship between the independent variables and the logarithm of the odds of the dependent variable. Logistic regression is widely used due to its simplicity, interpretability, and efficiency in processing large datasets.

```{python}
#| echo: false
#| error: true
#| warning: true

# Reshape y_train into a 1D array
y_train = np.ravel(y_train)

from sklearn.linear_model import LogisticRegression

from sklearn.preprocessing import StandardScaler

# Scale the input features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

reg = LogisticRegression(random_state = 42, max_iter=1000)
accuracies = cross_val_score(reg, X_train, y_train, cv=5)
reg.fit(X_train,y_train)
y_pred = reg.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",reg.score(X_test,y_test))


#Store results in the dictionaries
result_dict_train["Logistic Train Score"] = np.mean(accuracies)
result_dict_test["Logistic Test Score"] = reg.score(X_test,y_test)

```

\textcolor{blue}{2. K-Nearest Neighbors (KNN) Classifier}

KNN is a simple and intuitive algorithm used for both classification and regression tasks. It classifies a new data point by assigning it the majority class label among its k nearest neighbors in the feature space, where k is a `hyperparameter` chosen by the user. KNN operates on the principle that similar data points tend to belong to the same class. It is a non-parametric algorithm, which means it does not make explicit assumptions about the underlying data distribution.

```{python}
#| echo: false

knn = KNeighborsClassifier()
accuracies = cross_val_score(knn, X_train, y_train, cv=5)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",knn.score(X_test,y_test))


#Store results in the dictionaries
result_dict_train["KNN Train Score"] = np.mean(accuracies)
result_dict_test["KNN Test Score"] = knn.score(X_test,y_test)
```

\textcolor{blue}{3.  Support Vector Classifier}

SVM is a powerful supervised learning algorithm used for classification and regression tasks. In the case of regression, it is referred to as Support Vector Regression (SVR). SVR aims to find the `hyperplane` that best fits the data while maximizing the margin between different classes or, in the case of regression, minimizing the error between the predicted and actual values. It works by transforming the input data into a higher-dimensional space using a kernel function, allowing it to find a linear separation or regression boundary.

```{python}
#| echo: false

svc = SVC(random_state = 42)
accuracies = cross_val_score(svc, X_train, y_train, cv=5)
svc.fit(X_train,y_train)
y_pred = svc.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",svc.score(X_test,y_test))


#Store results in the dictionaries
result_dict_train["SVM Train Score"] = np.mean(accuracies)
result_dict_test["SVM Test Score"] = svc.score(X_test,y_test) 

```

\textcolor{blue}{4.  Decision Tree Classifier}

Decision tree regression is a non-parametric supervised learning algorithm used for regression tasks. It works by recursively partitioning the feature space into regions, with each region associated with a specific prediction value. The decision tree learns from the data by selecting the feature that best splits the dataset at each node based on certain criteria (e.g., minimizing variance or mean squared error). Decision trees are easy to interpret and visualize, making them useful for understanding the decision-making process of the model.

```{python}
#| echo: false

dtc = DecisionTreeClassifier(random_state = 42)
accuracies = cross_val_score(dtc, X_train, y_train, cv=5)
dtc.fit(X_train,y_train)
y_pred = dtc.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",dtc.score(X_test,y_test))



#Store results in the dictionaries
result_dict_train["Decision Tree Train Score"] = np.mean(accuracies)
result_dict_test["Decision Tree Test Score"] = dtc.score(X_test,y_test)
```

\textcolor{blue}{5.  Random Forest Classifier}

Random forest is an ensemble learning algorithm that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. In the case of regression, it aggregates the predictions of individual decision trees to produce a final prediction. Random forest introduces randomness both in the selection of features at each node and the bootstrap sampling of the training data. This randomness helps to decorrelate the individual trees, leading to more robust and generalizable predictions.

```{python}
#| echo: false
#| warning: false
#| message: false


rfc = RandomForestClassifier(random_state = 42)
accuracies = cross_val_score(rfc, X_train, y_train, cv=5)
rfc.fit(X_train,y_train)
y_pred = rfc.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",rfc.score(X_test,y_test))


#Store results in the dictionaries
result_dict_train["Random Forest Train Score"] = np.mean(accuracies)
result_dict_test["Random Forest Test Score"] = rfc.score(X_test,y_test)

```

\textcolor{blue}{6.  Naïve Bayes Classifier}

Naïve Bayes is a probabilistic classifier based on Bayes' theorem with the assumption of independence between features. Despite its simplicity, it is effective in many real-world classification tasks, especially in text categorization and spam filtering. Naïve Bayes calculates the probability of each class given the input features and selects the class with the highest probability as the predicted class label. Despite its simplifying assumptions, Naïve Bayes can perform surprisingly well, particularly on datasets with high dimensionality and sparse features.

```{python}
#| echo: false

gnb = GaussianNB()
accuracies = cross_val_score(gnb, X_train, y_train, cv=5)
gnb.fit(X_train,y_train)
y_pred = gnb.predict(X_test)
 
#Obtain accuracy
print("Train Score:",np.mean(accuracies))
print("Test Score:",gnb.score(X_test,y_test))


#Store results in the dictionaries
result_dict_train["Gaussian NB Train Score"] = np.mean(accuracies)
result_dict_test["Gaussian NB Test Score"] = gnb.score(X_test,y_test)

```

\textcolor{blue}{Select Algorithm Train and Test Accuracy Scores}

Tables and plots are used to visualize the performance of each algorithm and compare their effectiveness in predicting the white wine quality.

```{python}
#| echo: false

df_result_train = pd.DataFrame.from_dict(result_dict_train,orient = "index", columns=["Score"])
df_result_train

```

```{python}
#| echo: false

df_result_test = pd.DataFrame.from_dict(result_dict_test,orient = "index",columns=["Score"])
df_result_test
```

\textcolor{blue}{Visualizing the scores}

```{python}
#| echo: false
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize=(20, 5))
sns.barplot(x=df_result_train.index, y=df_result_train.Score, ax=ax[0])
sns.barplot(x=df_result_test.index, y=df_result_test.Score, ax=ax[1])

# Set ticks and labels
ax[0].set_xticks(range(len(df_result_train.index)))
ax[0].set_xticklabels(df_result_train.index, rotation=75)
ax[1].set_xticks(range(len(df_result_test.index)))
ax[1].set_xticklabels(df_result_test.index, rotation=75)

plt.show()

```

\textcolor{blue}{Conclusion:}

Random Forest Regression algorithm is the best machine learning algorithm for predicting the white wine quality.

\textcolor{blue}{Code:}

The code used for data preprocessing, model training, and evaluation will be provided in a separate Quarto Document file for reproducibility.
